{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "# Static values\n",
    "class static:\n",
    "    MACHINES = 33\n",
    "    LOCATIONS = 8\n",
    "    EMPLOYEES = 29\n",
    "    USERS = 23\n",
    "    TASKSIZE = 9250\n",
    "    JOBS = 39\n",
    "    TIMESHEETS = 2250\n",
    "    START_DATE = datetime.date(2021, 6, 1)\n",
    "    DATE_RANGE = 120\n",
    "    PROD_LOW_END = 58\n",
    "    PROD_HIGH_END = 105\n",
    "    DUMMY_PRODUCTIVITY_HIGH = [95, 95, 95, 100,\n",
    "                            100, 100, 100, 100, 100, 105]\n",
    "    DUMMY_PRODUCTIVITY_AVG = [50, 50, 50, 52, 55, 57, 60, 65, 65,\n",
    "                          70, 75, 75, 79, 80, 80, 80, 82, 89, \n",
    "                          92, 92, 92, 93, 93, 93, 96, 96, 96,\n",
    "                          96, 98, 98, 98, 100, 100, 100, 98,\n",
    "                          98, 98, 100, 100, 100, 100, 102,\n",
    "                          102, 102, 105, 105, 105, 110, 110]\n",
    "    DUMMY_PRODUCTIVITY_LOW = [30, 35, 35, 45, 45, 50, 50, 50,\n",
    "                            60, 60, 65, 70, 70, 75, 75, 80, 90,\n",
    "                            90, 95, 95, 100, 100, 100, 100, 100]\n",
    "\n",
    "# Converts list of tuples into CSV file\n",
    "# List in format [(1, 'Dummy Data', 'Test', 3), (2, 'Real Data', 'Hi Mum', 22)]\n",
    "# Converted to:\n",
    "# 1, 'Dummy Data', 'Test', 3\n",
    "# 2, 'Real Data', 'Hi Mum', 22\n",
    "\n",
    "def write2csv(file, data, header):\n",
    "    f = open(file, 'w', newline='')\n",
    "    obj = csv.writer(f)\n",
    "    obj.writerow(header)\n",
    "    obj.writerows(data)\n",
    "    f.close()\n",
    "\n",
    "# hash salt generator\n",
    "def generate_salt():\n",
    "    ALPHABET = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    chars = []\n",
    "    for i in range(16):\n",
    "        chars.append(random.choice(ALPHABET))\n",
    "    return \"\".join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECT: MACHINE\n",
    "# machine_id,desc,location_id\n",
    "\n",
    "data = []\n",
    "for x in range(1, static.MACHINES+1):\n",
    "    data.append(\n",
    "        tuple([\n",
    "            x,\n",
    "            'Machine '+str(x),\n",
    "            randint(1, static.LOCATIONS)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "write2csv('machines.csv', data, ['machine_id', 'desc', 'location_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECT: LOCATION\n",
    "# location_id, desc\n",
    "\n",
    "data = []\n",
    "for x in range(1, static.LOCATIONS+1):\n",
    "    data.append(\n",
    "        tuple([\n",
    "            x,\n",
    "            'Room '+str(x)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "write2csv('locations.csv', data, ['location_id', 'desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECT: USER_TYPE\n",
    "# type_id, desc, level, status\n",
    "\n",
    "data = [(1, \"Welder\", 1, 1), (2, \"Supervisor\", 2, 1), (3, \"Director\", 2, 1), (10, \"Admin\", 5, 1)]\n",
    "\n",
    "write2csv('user_types.csv', data, ['type_id', 'desc', 'level', 'status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECT: PAYROLL_USER (SmartFab RDBMS)\n",
    "# payroll_id, first_name, last_name, employment_status\n",
    "\n",
    "data = []\n",
    "first_names = ['Michael', 'Christopher', 'Amanda', 'Jason', 'Jessica', 'David', 'Melissa', 'James', 'Matthew', 'Joshua', 'John', 'Robert', 'Joseph', 'Michelle', 'Daniel', 'Scott', 'Susan', 'Keith', 'Mohammed', 'Maria', 'Nushi', 'Jose', 'Wei', 'Ahmed', 'Muhammad', 'Ali', 'Yan', 'Li', 'Ana', 'Ying', 'Juan', 'Jean', 'Carlos', 'Antonio', 'Hui', 'Elena']\n",
    "last_names = ['Smith', 'Smith', 'Johnson', 'Williams', 'Brown', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodrigeuz', 'Rodrigeuz', 'Martinez', 'Hernadez', 'Lopez', 'Wilson', 'Anderson', 'Thomas', 'Taylor', 'Moore', 'Jackson', 'Martin', 'Lee', 'Thompson', 'Martin', 'Lee', 'Thompson', 'White', 'Harris', 'Clark', 'Lewis', 'Robinson', 'Walker', 'Allen', 'Hill', 'Nguyen', 'Adams', 'Morales', 'Peterson', 'Patel', 'Kim', 'Choi', 'Jeong', 'de Silva', 'Perera', 'Fernando', 'Kumara', 'Chan', 'Wong', 'Lei', 'Kaya',\n",
    "              'Demir', 'Tran', 'Wang', 'Li', 'Zhang', 'Chen', 'Liu', 'Devi', 'Yang', 'Huang', 'Singh', 'Wu', 'Wu', 'Kumar', 'Xu', 'Zhao', 'Luo', 'Ray', 'Mitchell', 'Carrillo', 'Hayat', 'Oaron', 'Khalaf', 'Amir', 'Carter', 'Swain', 'Mallik', 'Carter', 'Swain', 'Mallik', 'de Carvalho', 'Parra', 'Stewart', 'Nasir', 'Verma', 'Kanwar', 'Nasir', 'Verma', 'Kanwar', 'Antonio', 'Nair', 'Celik', 'Pedro', 'Bell', 'Lucas', 'Fischer', 'Wood', 'Meza', 'Hansen', 'Jang', 'Tahir', 'Cook', 'Watson', 'Rogers', 'Ward', 'Roman', 'Richards', 'Manna', 'Bailey']\n",
    "\n",
    "for x in range(1, static.EMPLOYEES):\n",
    "    fn = first_names[randint(0, len(first_names)-1)]\n",
    "    ln = last_names[randint(0, len(last_names)-1)]\n",
    "    data.append(\n",
    "        tuple([\n",
    "            format(randint(1, 999999), '06'),\n",
    "            fn,\n",
    "            ln,\n",
    "            1\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    last_names.remove(ln)\n",
    "\n",
    "write2csv('payroll_users.csv', data, ['payroll_id', 'first_name', 'last_name', 'employment_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBJECT: TIMESHEET (SmartFab RDBMS)\n",
    "# timesheet_id, payroll_id, date, shift_start, shift_end\n",
    "\n",
    "data, pids, dates = [], [], []\n",
    "shifts = [('08:00', '16:00'), ('09:00', '13:00'), ('09:00', '17:00'), ('09:30', '16:30'), ('12:00', '17:00'), ('12:30', '18:00')]\n",
    "\n",
    "# Generate payroll ids\n",
    "with open('payroll_users.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    for r in l:\n",
    "        try:\n",
    "            pids.append(int(r[0]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Generate dates\n",
    "for x in range(1,static.TIMESHEETS):\n",
    "    random_date = static.START_DATE + datetime.timedelta(days=random.randrange(static.DATE_RANGE))\n",
    "    dates.append(random_date)\n",
    "dates = sorted(dates)\n",
    "\n",
    "emp_count = 0\n",
    "for x in dates:\n",
    "    try:\n",
    "        employee = pids[emp_count]\n",
    "    except:\n",
    "        emp_count = 0\n",
    "        employee = pids[emp_count]\n",
    "    emp_count += 1\n",
    "    \n",
    "    shift = shifts[randint(0, len(shifts)-1)]\n",
    "    random.seed(emp_count)\n",
    "    tid = hashlib.md5((str(employee) + str(shift) + str(x) + str(randint(emp_count,int(employee)))).encode('utf-8')).hexdigest()\n",
    "    data.append(\n",
    "        tuple([\n",
    "            tid, #timesheet ID generated MD5 hash\n",
    "            int(employee), #payroll_id\n",
    "            x, # timesheet date\n",
    "            shift[0], #start time\n",
    "            shift[1], #end time\n",
    "        ])\n",
    "    )\n",
    "\n",
    "write2csv('timesheet_data.csv', data, ['timesheet_id', 'payroll_id', 'date', 'shift_start', 'shift_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT: TIMESHEET_ENTRY\n",
    "# tid, org_timesheet_id, payroll_id, date, shift_start, shift_end, shift_length\n",
    "\n",
    "data = []\n",
    "id_counter = 1\n",
    "with open('timesheet_data.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    for r in l:\n",
    "        try:\n",
    "            start_hr, start_min = int(re.split('[:]',r[3])[0]), int(re.split('[:]',r[3])[1])\n",
    "            end_hr, end_min = int(re.split('[:]',r[4])[0]), int(re.split('[:]',r[4])[1])\n",
    "            shift_len = ((datetime.timedelta(hours=end_hr, minutes=end_min) - datetime.timedelta(hours=start_hr, minutes=start_min)).total_seconds() / 3600)\n",
    "\n",
    "            data.append(\n",
    "                tuple([\n",
    "                    id_counter, #tid\n",
    "                    r[0], #original timesheet ID\n",
    "                    r[1], #payroll ID\n",
    "                    r[2], #timesheet date\n",
    "                    r[3], #shift start\n",
    "                    r[4], #shift end\n",
    "                    shift_len, #shift length\n",
    "                ])\n",
    "            )\n",
    "\n",
    "            id_counter += 1\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "write2csv('timesheets.csv', data, ['tid', 'org_timesheet_id', 'payroll_id', 'date', 'shift_start', 'shift_end', 'shift_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT: USER_LIST\n",
    "# payroll_id, first_name, last_name, employment_status\n",
    "\n",
    "data = []\n",
    "with open('payroll_users.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    for r in l:\n",
    "        try:\n",
    "            data.append(\n",
    "                tuple([\n",
    "                    int(r[0]), #payroll_id\n",
    "                    r[1], #first_name\n",
    "                    r[2], #last_name\n",
    "                    int(r[3]), #employment_status\n",
    "                ])\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "write2csv('user_list.csv', data, ['payroll_id', 'first_name', 'last_name', 'employment_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT: USER\n",
    "# uid, payroll_id, first_name, last_name, status, timesheet_status, user_types\n",
    "\n",
    "data = [(1, 1000001, 'Lachlan', 'Langmead', 1, 0, [1, 2, 3, 10]), (2, 1000002, 'Mark', 'Lee', 1, 0, [1, 2, 3, 10])] #Add Developers\n",
    "users = []\n",
    "user_sample = random.sample(range(3,static.EMPLOYEES-1), static.USERS)\n",
    "random_types = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3] # weight role to be more likely to select user type '1' (Welder)\n",
    "\n",
    "# Read current user list into list\n",
    "with open('user_list.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    for r in l:\n",
    "        try:\n",
    "            users.append(\n",
    "                tuple([\n",
    "                    int(r[0]), #payroll_id\n",
    "                    r[1], #first_name\n",
    "                    r[2], #last_name\n",
    "                    int(r[3]), #employment_status\n",
    "                ])\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "for x in user_sample:\n",
    "    role = random_types[randint(0, len(random_types)-1)]\n",
    "    data.append(\n",
    "        tuple([\n",
    "            x, #uid\n",
    "            users[x][0], #payroll_id\n",
    "            users[x][1], #first_name\n",
    "            users[x][2], #last_name\n",
    "            users[x][3], #employment_status\n",
    "            1 if role == 1 or role == 2 else 0, #timesheet_status,\n",
    "            [role]\n",
    "        ])\n",
    "    )\n",
    "\n",
    "write2csv('users.csv', data, ['uid', 'payroll_id', 'first_name', 'last_name', 'status', 'timesheet_status', 'user_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT: CATEGORY\n",
    "# category_id, arctime\n",
    "\n",
    "data = [\n",
    "    (1, 10),\n",
    "    (2, 20),\n",
    "    (3, 30),\n",
    "    (4, 40),\n",
    "    (5, 50)\n",
    "]\n",
    "\n",
    "write2csv('categories.csv', data, ['category_id', 'arctime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT: JOB\n",
    "# job_id, category, start_date, end_date, total_hrs\n",
    "data = []\n",
    "jids = random.sample(range(7000, 9000), static.JOBS)\n",
    "jids = sorted(jids)\n",
    "\n",
    "for x in range(1, len(jids)):\n",
    "    random_date = static.START_DATE + datetime.timedelta(days=random.randrange(60))\n",
    "    dates.append(random_date)\n",
    "dates = sorted(dates)\n",
    "\n",
    "categories = []\n",
    "with open('categories.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    next(f, None) # skip headers\n",
    "    for r in l:\n",
    "        categories.append(int(r[0]))\n",
    "\n",
    "for x in jids:\n",
    "    category = categories[randint(0, len(categories)-1)] # random category\n",
    "    start_date = dates[randint(0, len(dates)-1)] # random start date\n",
    "\n",
    "    # random end date based on category, smaller jobs = less time to take, with 70% chance of being finished\n",
    "    end_date = (start_date + datetime.timedelta(days=random.randrange(category))) if randint(1,10) < 7 else None\n",
    "\n",
    "    # random job arctime lengths\n",
    "    job_lengths = [25, 25, 30, 50, 50, 50, 50, 75, 75, 85, 90, 90, 100, 100, 100, 100, 110, 125, 125, 150, 200, 200, 250, 275, 275, 300] \n",
    "    data.append(\n",
    "        tuple([\n",
    "            x, #job id\n",
    "            category, # random category\n",
    "            start_date, #start date\n",
    "            0, #end date, all zero to begin with\n",
    "            0,\n",
    "            job_lengths[randint(0, len(job_lengths)-1)]\n",
    "        ])\n",
    "    )\n",
    "\n",
    "write2csv('jobs.csv', data, ['job_id', 'category', 'start_date', 'end_date', 'accum_hrs', 'total_hrs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT: TASK_TYPE\n",
    "# task_type_id, desc\n",
    "\n",
    "data = [\n",
    "    (100, \"login\"),\n",
    "    (101, \"logout\"),\n",
    "    (404, \"unknown error\")\n",
    "]\n",
    "\n",
    "write2csv('task_types.csv', data, ['task_type_id', 'desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add2log(user_id, type_id, job_id, machine_id, dt):\n",
    "    #task id generator\n",
    "    tid = hashlib.md5((\n",
    "        str(user_id) + # User ID\n",
    "        str(dt) + # Clock in time\n",
    "        str(datetime.datetime.now()) + # Current System Time\n",
    "        str(machine_id) + # Machine ID of login machine\n",
    "        generate_salt() # Random Salt\n",
    "        ).encode('utf-8')).hexdigest()\n",
    "    \n",
    "    # task_id, uid, type, job_id, machine_id, datetime\n",
    "    # IoT Login = 100\n",
    "    return tuple([\n",
    "            tid, #task id\n",
    "            user_id, #user id\n",
    "            type_id, #type = 100 = login\n",
    "            job_id, #job id\n",
    "            machine_id, #machine id\n",
    "            dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        ])\n",
    "        \n",
    "\n",
    "def find_job(d):\n",
    "    jids, data = [], []\n",
    "    found = None\n",
    "    date_dt = datetime.datetime.strptime(d, '%Y-%m-%d')\n",
    "    with open('jobs.csv', newline='') as f:\n",
    "        next(f, None)  # skip headers\n",
    "        l = csv.reader(f, delimiter=',')\n",
    "        for r in l:\n",
    "            if (r[3] != '0'):\n",
    "                continue\n",
    "\n",
    "            start_date = datetime.datetime.strptime(r[2], '%Y-%m-%d')\n",
    "            try:\n",
    "                end_date = datetime.datetime.strptime(r[3], '%Y-%m-%d')\n",
    "            except:\n",
    "                end_date = datetime.datetime.today()\n",
    "\n",
    "            if (date_dt >= start_date and date_dt <= end_date):\n",
    "                jids.append(\n",
    "                    tuple([\n",
    "                        r[0],\n",
    "                        r[1],\n",
    "                        r[2],\n",
    "                        r[3],\n",
    "                        r[4]\n",
    "                    ])\n",
    "                )\n",
    "\n",
    "    if len(jids) > 0:\n",
    "        return jids[randint(0, len(jids)-1)]\n",
    "    else:\n",
    "        return [-1]\n",
    "\n",
    "\n",
    "def inc_job_hrs(jid, hrs, date):\n",
    "    f = open('jobs.csv')\n",
    "    r = csv.reader(f)\n",
    "    next(r, None)  # skip headers\n",
    "    lines = list(r)\n",
    "    count = 0\n",
    "\n",
    "    while count < len(lines):\n",
    "        if int(lines[count][0]) == jid:\n",
    "            new_hrs = float(lines[count][4]) + hrs\n",
    "            lines[count][4] = new_hrs\n",
    "            if (float(lines[count][4]) >= float(lines[count][5])):\n",
    "                if (randint(0, 10) > 8): # 80% chance of going over accum hours\n",
    "                    lines[count][3] = str(date)[:10] # reset enddate to date of shift\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    f.close()\n",
    "    f = open('jobs.csv', 'w', newline='')\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['job_id', 'category', 'start_date',\n",
    "                'end_date', 'accum_hrs', 'total_hrs'])\n",
    "    w.writerows(lines)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#OBJECT: TASK\n",
    "# task_id, uid, type, job_id, machine_id, datetime\n",
    "\n",
    "data, uids, mids = [], [], []\n",
    "\n",
    "# Generate task ids from random sample\n",
    "temp_list = list(range(1, (static.TASKSIZE*2)))\n",
    "task_id_list = [\n",
    "    temp_list[i] for i in sorted(random.sample(range(len(temp_list)), static.TASKSIZE))\n",
    "]\n",
    "\n",
    "# Build user ids\n",
    "with open('user_list.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    for r in l:\n",
    "        try:\n",
    "            uids.append(int(r[0]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "user_ratings = []\n",
    "for u in uids:\n",
    "    user_ratings.append(tuple([\n",
    "        u,\n",
    "        randint(1, 10)\n",
    "    ]))\n",
    "\n",
    "# Build machine ids\n",
    "with open('machines.csv', newline='') as f:\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    for r in l:\n",
    "        try:\n",
    "            mids.append(int(r[0]))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "uids = uids[2:] # Remove developer IDs\n",
    "\n",
    "master_mids = mids.copy()\n",
    "current_day = None\n",
    "\n",
    "with open('timesheets.csv', newline='') as f:\n",
    "    next(f, None) # skip headers\n",
    "    l = csv.reader(f, delimiter=',')\n",
    "    new_mid_list = False\n",
    "    for r in l:\n",
    "\n",
    "        chance_of_shift = True if randint(0,100)<99 else False # Chance of shift is 99%, chances of calling in sick/no-shows/holidays/etc\n",
    "        \n",
    "        if (datetime.datetime.strptime(r[3], '%Y-%m-%d') != current_day):\n",
    "            mids = master_mids.copy()\n",
    "\n",
    "        current_day = datetime.datetime.strptime(r[3], '%Y-%m-%d')\n",
    "\n",
    "        if (chance_of_shift):\n",
    "\n",
    "            current_user = int(r[2])\n",
    "\n",
    "            current_job_master = find_job(r[3])\n",
    "            current_job = int(current_job_master[0])  # job id\n",
    "\n",
    "            if (current_job == -1):\n",
    "                continue\n",
    "\n",
    "            current_job_arctime = (int(current_job_master[1])*10)\n",
    "\n",
    "            try:\n",
    "                current_machine = mids[randint(0, len(mids)-1)]\n",
    "                mids.remove(current_machine)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            shift_length = float(r[6])\n",
    "            start_hr, start_min = int(re.split('[:]',r[4])[0]), int(re.split('[:]',r[4])[1])\n",
    "            end_hr, end_min = int(re.split('[:]',r[5])[0]), int(re.split('[:]',r[5])[1])\n",
    "            date = [int(x) for x in r[3].split('-')]\n",
    "            shift_year, shift_month, shift_day = date\n",
    "\n",
    "            dt_start = datetime.datetime(shift_year, shift_month, shift_day, start_hr, start_min, 0)\n",
    "            dt_end = datetime.datetime(shift_year, shift_month, shift_day, end_hr, end_min, 0)\n",
    "\n",
    "            ############# PADDING AMOUNTS #################\n",
    "            ### Factors in random lost time events/late arrival/early start/etc\n",
    "\n",
    "            # Actual time expected worked = arc_shift\n",
    "            # Adjusting shift = arc_padding\n",
    "            # Shift_length (in hrs) x (expected arctime/60)\n",
    "            \n",
    "            arc_shift = shift_length * (current_job_arctime/60)\n",
    "\n",
    "            prod_padding_chance = [i[1] for i in user_ratings if i[0] == current_user][0]\n",
    "\n",
    "            if (prod_padding_chance <= 2):\n",
    "                prod_padding = static.DUMMY_PRODUCTIVITY_LOW[randint(0, len(static.DUMMY_PRODUCTIVITY_LOW)-1)]/100\n",
    "\n",
    "            elif ((prod_padding_chance <= 5) and (prod_padding_chance > 2)):\n",
    "                prod_padding = static.DUMMY_PRODUCTIVITY_AVG[randint(0, len(static.DUMMY_PRODUCTIVITY_AVG)-1)]/100\n",
    "\n",
    "            else:\n",
    "                prod_padding = static.DUMMY_PRODUCTIVITY_HIGH[randint(0, len(static.DUMMY_PRODUCTIVITY_HIGH)-1)]/100\n",
    "            \n",
    "            padded_arc_shift = arc_shift * prod_padding\n",
    "\n",
    "            shift_padding = (shift_length - padded_arc_shift)\n",
    "\n",
    "            split_padding = (shift_padding)/2\n",
    "\n",
    "            m = split_padding * 60\n",
    "\n",
    "            dt_start = dt_start + datetime.timedelta(minutes=m)\n",
    "\n",
    "            dt_end = dt_end - datetime.timedelta(minutes=m)\n",
    "\n",
    "            # log in\n",
    "            log_record = add2log(current_user, 100, current_job, current_machine, dt_start)\n",
    "            data.append(log_record)\n",
    "\n",
    "            #log out\n",
    "            log_record = add2log(current_user, 101, current_job, current_machine, dt_end)\n",
    "            data.append(log_record)\n",
    "\n",
    "            dt_length = dt_end - dt_start # calculate real shift length\n",
    "\n",
    "            #print(\"end: \" + str(dt_end) + \" start:\" + str(dt_start) + \" = \" + str(dt_length))\n",
    "            (h, m, s) = str(dt_length).split(':')\n",
    "\n",
    "            decimal_time = int(h) + (int(m) / 60) + (int(s) / 3600) # convert datetime to decimal shift length\n",
    "\n",
    "            inc_job_hrs(current_job, decimal_time, current_day) #increment worked hours into jobs.csv\n",
    "\n",
    "            ## Chance to add an Unknown record\n",
    "            if randint(0, 1000) == 1:\n",
    "                log_record = add2log(current_user, 404, randint(0, 99999), randint(1, 500), dt_start)\n",
    "                data.append(log_record)\n",
    "\n",
    "\n",
    "# task_id, uid, type, job_id, machine_id, datetime\n",
    "write2csv('log.csv', data, ['task_id', 'user', 'type', 'job', 'machine', 'datetime'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1d92c2b884777ef43502feec02fe6e69a490113cc87137b976695f29e9a4289"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
